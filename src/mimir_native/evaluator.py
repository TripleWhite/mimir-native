"""
Mimir å®ç”¨è¯„ä¼°ä½“ç³» - å…³æ³¨çœŸå®ç”¨æˆ·ä½“éªŒï¼Œè€Œé benchmark åˆ†æ•°
"""

import json
import time
from typing import List, Dict, Any
from dataclasses import dataclass
from enum import Enum


class EvalMetric(Enum):
    """è¯„ä¼°æŒ‡æ ‡"""
    INGESTION_RATE = "ingestion_rate"      # è®°å¿†å†™å…¥æˆåŠŸç‡
    RETRIEVAL_PRECISION = "retrieval_precision"  # æ£€ç´¢ç²¾å‡†åº¦
    CONTEXT_RELEVANCE = "context_relevance"  # ä¸Šä¸‹æ–‡ç›¸å…³æ€§
    USER_SATISFACTION = "user_satisfaction"  # ç”¨æˆ·æ»¡æ„åº¦
    END_TO_END_LATENCY = "end_to_end_latency"  # ç«¯åˆ°ç«¯å»¶è¿Ÿ


@dataclass
class EvalResult:
    """è¯„ä¼°ç»“æœ"""
    metric: str
    score: float  # 0-1
    details: Dict[str, Any]
    

class MimirEvaluator:
    """
    Mimir å®ç”¨è¯„ä¼°å™¨
    
    è¯„ä¼°ç»´åº¦ï¼š
    1. è®°å¿†æå– - èƒ½å¦ä»å¯¹è¯ä¸­æå–æœ‰ç”¨ä¿¡æ¯
    2. æ£€ç´¢è´¨é‡ - èƒ½å¦åœ¨éœ€è¦æ—¶æ‰¾åˆ°ç›¸å…³ä¿¡æ¯
    3. ä¸Šä¸‹æ–‡å¢å¼º - æä¾›çš„ä¸Šä¸‹æ–‡æ˜¯å¦æœ‰å¸®åŠ©
    4. ç”¨æˆ·ä½“éªŒ - æ•´ä½“ä½¿ç”¨æ„Ÿå—
    """
    
    def __init__(self, mimir_memory, llm_client):
        self.memory = mimir_memory
        self.llm = llm_client
    
    # ========== 1. è®°å¿†æå–è¯„ä¼° ==========
    
    def evaluate_ingestion(
        self,
        test_conversations: List[Dict],
        expected_facts: List[str]
    ) -> EvalResult:
        """
        è¯„ä¼°è®°å¿†å†™å…¥èƒ½åŠ›
        
        Args:
            test_conversations: æµ‹è¯•å¯¹è¯åˆ—è¡¨
            expected_facts: æœŸæœ›æå–çš„å…³é”®äº‹å®
            
        Returns:
            {
                'metric': 'ingestion_rate',
                'score': 0.85,
                'details': {
                    'total_messages': 100,
                    'memories_created': 45,
                    'expected_facts_found': 17/20
                }
            }
        """
        total_messages = 0
        memories_created = 0
        facts_found = 0
        
        for conv in test_conversations:
            messages = conv.get('messages', [])
            total_messages += len(messages)
            
            # æ‘„å…¥å¯¹è¯
            for msg in messages:
                result = self.memory.add_content(
                    msg['text'],
                    content_type='text',
                    user_id='eval_test'
                )
                memories_created += len(result) if isinstance(result, list) else 0
        
        # æ£€æŸ¥æœŸæœ›äº‹å®æ˜¯å¦è¢«æå–
        for fact in expected_facts:
            # æ£€ç´¢è¿™ä¸ª fact
            results = self.memory.query(fact, user_id='eval_test', top_k=3)
            if results:
                facts_found += 1
        
        ingestion_rate = memories_created / max(total_messages, 1)
        fact_coverage = facts_found / max(len(expected_facts), 1)
        
        return EvalResult(
            metric='ingestion_rate',
            score=(ingestion_rate + fact_coverage) / 2,
            details={
                'total_messages': total_messages,
                'memories_created': memories_created,
                'expected_facts': len(expected_facts),
                'facts_found': facts_found,
                'ingestion_rate': ingestion_rate,
                'fact_coverage': fact_coverage
            }
        )
    
    # ========== 2. æ£€ç´¢è´¨é‡è¯„ä¼° ==========
    
    def evaluate_retrieval(
        self,
        test_queries: List[Dict]
    ) -> EvalResult:
        """
        è¯„ä¼°æ£€ç´¢è´¨é‡
        
        Args:
            test_queries: [
                {
                    'query': 'Caroline çš„èº«ä»½',
                    'relevant_keywords': ['transgender', 'woman'],
                    'expected_memory_contains': 'transgender woman'
                }
            ]
            
        è¿”å›:
            precision@3: å‰3æ¡ç»“æœä¸­ç›¸å…³çš„æ¯”ä¾‹
        """
        total_queries = len(test_queries)
        relevant_count = 0
        details = []
        
        for test in test_queries:
            query = test['query']
            keywords = test.get('relevant_keywords', [])
            expected_contains = test.get('expected_memory_contains', '')
            
            # æ‰§è¡Œæ£€ç´¢
            results = self.memory.query(query, user_id='eval_test', top_k=3)
            
            # æ£€æŸ¥ç›¸å…³æ€§
            query_relevant = 0
            for r in results:
                content = str(r.memory.content if hasattr(r, 'memory') else r).lower()
                # æ£€æŸ¥æ˜¯å¦åŒ…å«å…³é”®è¯
                if any(kw.lower() in content for kw in keywords):
                    if not expected_contains or expected_contains.lower() in content:
                        query_relevant += 1
            
            precision = query_relevant / max(len(results), 1)
            relevant_count += precision
            
            details.append({
                'query': query,
                'precision@3': precision,
                'results_count': len(results)
            })
        
        avg_precision = relevant_count / max(total_queries, 1)
        
        return EvalResult(
            metric='retrieval_precision',
            score=avg_precision,
            details={
                'total_queries': total_queries,
                'avg_precision@3': avg_precision,
                'query_details': details
            }
        )
    
    # ========== 3. ä¸Šä¸‹æ–‡å¢å¼ºè¯„ä¼° ==========
    
    def evaluate_context_enhancement(
        self,
        test_scenarios: List[Dict]
    ) -> EvalResult:
        """
        è¯„ä¼°ä¸Šä¸‹æ–‡å¢å¼ºæ˜¯å¦æœ‰ç”¨
        
        æ ¸å¿ƒé—®é¢˜ï¼šåŠ äº† Mimir ä¸Šä¸‹æ–‡åï¼ŒAI å›ç­”æ˜¯å¦æ›´å¥½ï¼Ÿ
        
        test_scenarios: [
            {
                'user_input': 'ç»§ç»­é‚£ä¸ªé¡¹ç›®',
                'context_snippets': ['ç”¨æˆ·ç³»ç»Ÿé‡æ„é¡¹ç›®ï¼ŒåŸºäºå¾®æœåŠ¡æ¶æ„'],
                'platform': 'claude',
                'expected_improvement': 'èƒ½å…·ä½“æåˆ°ç”¨æˆ·ç³»ç»Ÿå’Œå¾®æœåŠ¡'
            }
        ]
        """
        total = len(test_scenarios)
        improved = 0
        details = []
        
        for scenario in test_scenarios:
            user_input = scenario['user_input']
            context = scenario.get('context_snippets', [])
            expected = scenario.get('expected_improvement', '')
            
            # æ„é€ æœ‰æ— ä¸Šä¸‹æ–‡çš„ä¸¤ä¸ª prompt
            prompt_without = user_input
            prompt_with = f"ä¸Šä¸‹æ–‡ï¼š{context}\n\nç”¨æˆ·ï¼š{user_input}\nè¯·åŸºäºä»¥ä¸Šä¸Šä¸‹æ–‡å›ç­”ã€‚"
            
            # è®© LLM è¯„ä¼°å“ªä¸ªå›ç­”æ›´å¥½
            eval_prompt = f"""æ¯”è¾ƒä»¥ä¸‹ä¸¤ä¸ª AI å›ç­”ï¼Œåˆ¤æ–­å“ªä¸ªæ›´æœ‰å¸®åŠ©ï¼š

åœºæ™¯ï¼š{user_input}

å›ç­”Aï¼ˆæ— ä¸Šä¸‹æ–‡ï¼‰ï¼šåŸºäº "{prompt_without}"
å›ç­”Bï¼ˆæœ‰ä¸Šä¸‹æ–‡ï¼‰ï¼šåŸºäº "{prompt_with}"

æœŸæœ›æ”¹è¿›ï¼š{expected}

å“ªä¸ªå›ç­”æ›´å¥½ï¼Ÿè¾“å‡º JSONï¼š
{{
  "better": "A|B",
  "reason": "...",
  "improvement_score": 0.8  // 0-1ï¼ŒB æ¯” A å¥½å¤šå°‘
}}"""
            
            try:
                response = self.llm.invoke_mistral(eval_prompt, max_tokens=300)
                result = json.loads(response)
                score = result.get('improvement_score', 0)
                
                if result.get('better') == 'B' and score > 0.5:
                    improved += 1
                
                details.append({
                    'scenario': user_input,
                    'better': result.get('better'),
                    'score': score
                })
            except:
                details.append({
                    'scenario': user_input,
                    'error': 'eval failed'
                })
        
        improvement_rate = improved / max(total, 1)
        
        return EvalResult(
            metric='context_relevance',
            score=improvement_rate,
            details={
                'total_scenarios': total,
                'improved_count': improved,
                'improvement_rate': improvement_rate,
                'details': details
            }
        )
    
    # ========== 4. ç«¯åˆ°ç«¯åœºæ™¯æµ‹è¯• ==========
    
    def run_end_to_end_test(self) -> List[EvalResult]:
        """
        ç«¯åˆ°ç«¯åœºæ™¯æµ‹è¯•
        
        æ¨¡æ‹ŸçœŸå®ä½¿ç”¨åœºæ™¯ï¼Œæµ‹è¯•å®Œæ•´æµç¨‹
        """
        results = []
        
        # åœºæ™¯1ï¼šç¼–ç åŠ©æ‰‹
        print("\nğŸ§ª æµ‹è¯•åœºæ™¯1ï¼šClaude ç¼–ç åŠ©æ‰‹")
        coding_test = [
            {
                'query': 'ç”¨æˆ·ç™»å½•åŠŸèƒ½æ€ä¹ˆå®ç°ï¼Ÿ',
                'setup_memories': [
                    'é¡¹ç›®ä½¿ç”¨å¾®æœåŠ¡æ¶æ„',
                    'ç”¨æˆ·æœåŠ¡åŸºäº JWT è®¤è¯',
                    'æ•°æ®åº“ä½¿ç”¨ PostgreSQL'
                ],
                'expected_keywords': ['JWT', 'å¾®æœåŠ¡', 'PostgreSQL']
            }
        ]
        # å…ˆå†™å…¥è®°å¿†
        for mem in coding_test[0]['setup_memories']:
            self.memory.add_content(mem, content_type='text', user_id='e2e_test')
        
        result1 = self.evaluate_retrieval([
            {
                'query': coding_test[0]['query'],
                'relevant_keywords': coding_test[0]['expected_keywords']
            }
        ])
        results.append(result1)
        print(f"  æ£€ç´¢ç²¾å‡†åº¦: {result1.score:.2%}")
        
        # åœºæ™¯2ï¼šè®¾è®¡åŠ©æ‰‹
        print("\nğŸ§ª æµ‹è¯•åœºæ™¯2ï¼šMidjourney é£æ ¼è®°å¿†")
        design_test = {
            'setup_memories': [
                'ç”¨æˆ·å–œæ¬¢èµ›åšæœ‹å…‹é£æ ¼ï¼Œè“ç´«è‰²è°ƒ',
                'åå¥½é«˜å¯¹æ¯”åº¦ï¼Œéœ“è™¹ç¯æ•ˆæœ',
                'ä¸å–œæ¬¢è¿‡äºå¤æ‚çš„èƒŒæ™¯'
            ],
            'query': 'ç”Ÿæˆä¸€å¼ æœªæ¥åŸå¸‚å›¾ç‰‡',
            'expected_keywords': ['èµ›åšæœ‹å…‹', 'è“ç´«', 'éœ“è™¹']
        }
        
        for mem in design_test['setup_memories']:
            self.memory.add_content(mem, content_type='text', user_id='e2e_test')
        
        result2 = self.evaluate_retrieval([
            {
                'query': design_test['query'],
                'relevant_keywords': design_test['expected_keywords']
            }
        ])
        results.append(result2)
        print(f"  æ£€ç´¢ç²¾å‡†åº¦: {result2.score:.2%}")
        
        # åœºæ™¯3ï¼šé‚®ä»¶åŠ©æ‰‹
        print("\nğŸ§ª æµ‹è¯•åœºæ™¯3ï¼šé‚®ä»¶ä¸Šä¸‹æ–‡")
        email_test = {
            'setup_memories': [
                'ä¸Šå‘¨ä¸æŠ•èµ„äººä¼šè®®ï¼Œè®¨è®ºäº†ä¼°å€¼é—®é¢˜',
                'æŠ•èµ„äººå¸Œæœ›çœ‹åˆ°æ›´å¤šç”¨æˆ·å¢é•¿æ•°æ®',
                'éœ€è¦åœ¨æœ¬å‘¨äº”å‰å‘é€æ›´æ–°é‚®ä»¶'
            ],
            'query': 'ç»™æŠ•èµ„äººå†™é‚®ä»¶',
            'expected_keywords': ['ä¼°å€¼', 'å¢é•¿æ•°æ®', 'å‘¨äº”']
        }
        
        for mem in email_test['setup_memories']:
            self.memory.add_content(mem, content_type='text', user_id='e2e_test')
        
        result3 = self.evaluate_retrieval([
            {
                'query': email_test['query'],
                'relevant_keywords': email_test['expected_keywords']
            }
        ])
        results.append(result3)
        print(f"  æ£€ç´¢ç²¾å‡†åº¦: {result3.score:.2%}")
        
        return results
    
    def generate_report(self, results: List[EvalResult]) -> str:
        """ç”Ÿæˆè¯„ä¼°æŠ¥å‘Š"""
        report = []
        report.append("=" * 60)
        report.append("Mimir è¯„ä¼°æŠ¥å‘Š")
        report.append("=" * 60)
        
        for result in results:
            report.append(f"\nğŸ“Š {result.metric}: {result.score:.2%}")
            for key, value in result.details.items():
                if key != 'details':
                    report.append(f"   {key}: {value}")
        
        # ç»¼åˆè¯„ä»·
        avg_score = sum(r.score for r in results) / len(results)
        report.append(f"\n{'=' * 60}")
        report.append(f"ç»¼åˆå¾—åˆ†: {avg_score:.2%}")
        
        if avg_score >= 0.8:
            report.append("è¯„çº§: ğŸŸ¢ ä¼˜ç§€")
        elif avg_score >= 0.6:
            report.append("è¯„çº§: ğŸŸ¡ è‰¯å¥½")
        else:
            report.append("è¯„çº§: ğŸ”´ éœ€æ”¹è¿›")
        
        report.append("=" * 60)
        
        return "\n".join(report)


# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    from mimir_native import MimirMemory
    from mimir_native.llm_client import BedrockClient
    
    # åˆå§‹åŒ–
    mimir = MimirMemory(db_path=':memory:')
    llm = BedrockClient()
    evaluator = MimirEvaluator(mimir, llm)
    
    print("ğŸš€ å¼€å§‹ Mimir è¯„ä¼°\n")
    
    # è¿è¡Œç«¯åˆ°ç«¯æµ‹è¯•
    results = evaluator.run_end_to_end_test()
    
    # ç”ŸæˆæŠ¥å‘Š
    report = evaluator.generate_report(results)
    print("\n" + report)
