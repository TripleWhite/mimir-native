#!/usr/bin/env python3
"""
LoCoMo Hybrid Retriever - æ‰¹å¤„ç† + ç¼“å­˜ç‰ˆæœ¬
"""

import json
import sys
import os
import re
import math
import hashlib
from datetime import datetime, timedelta
from typing import List, Dict, Any, Optional, Tuple
import requests


class CachedEmbeddingClient:
    """å¸¦ç¼“å­˜çš„ Embedding å®¢æˆ·ç«¯"""
    
    def __init__(self, api_key: str, base_url: str = "https://llmapi.paratera.com", 
                 cache_file: str = "embeddings_cache.json"):
        self.api_key = api_key
        self.base_url = base_url.rstrip('/')
        self.embed_url = f"{self.base_url}/v1/embeddings"
        self.cache_file = cache_file
        self.cache = self._load_cache()
    
    def _load_cache(self) -> Dict:
        """åŠ è½½ç¼“å­˜"""
        if os.path.exists(self.cache_file):
            try:
                with open(self.cache_file, 'r') as f:
                    return json.load(f)
            except:
                pass
        return {}
    
    def _save_cache(self):
        """ä¿å­˜ç¼“å­˜"""
        with open(self.cache_file, 'w') as f:
            json.dump(self.cache, f)
    
    def _get_text_hash(self, text: str) -> str:
        """è·å–æ–‡æœ¬å“ˆå¸Œ"""
        return hashlib.md5(text.encode()).hexdigest()
    
    def get_embedding(self, text: str) -> List[float]:
        """è·å–å•ä¸ªæ–‡æœ¬çš„ embeddingï¼ˆå¸¦ç¼“å­˜ï¼‰"""
        text_hash = self._get_text_hash(text)
        
        # æ£€æŸ¥ç¼“å­˜
        if text_hash in self.cache:
            return self.cache[text_hash]
        
        # è°ƒç”¨ API
        headers = {
            "Authorization": f"Bearer {self.api_key}",
            "Content-Type": "application/json"
        }
        
        payload = {
            "model": "GLM-Embedding-3",
            "input": text[:512]
        }
        
        try:
            response = requests.post(
                self.embed_url,
                headers=headers,
                json=payload,
                timeout=30
            )
            response.raise_for_status()
            result = response.json()
            
            if "data" in result and len(result["data"]) > 0:
                embedding = result["data"][0]["embedding"]
                # ä¿å­˜åˆ°ç¼“å­˜
                self.cache[text_hash] = embedding
                return embedding
        except Exception as e:
            print(f"  Embedding error: {e}")
        
        return []
    
    def get_embeddings_batch(self, texts: List[str], batch_size: int = 32) -> List[List[float]]:
        """æ‰¹é‡è·å– embeddings"""
        results = []
        uncached_texts = []
        uncached_indices = []
        
        # å…ˆæ£€æŸ¥ç¼“å­˜
        for i, text in enumerate(texts):
            text_hash = self._get_text_hash(text)
            if text_hash in self.cache:
                results.append((i, self.cache[text_hash]))
            else:
                results.append((i, None))
                uncached_texts.append(text)
                uncached_indices.append(i)
        
        # æ‰¹é‡è°ƒç”¨ API è·å–æœªç¼“å­˜çš„
        if uncached_texts:
            print(f"  APIè°ƒç”¨: {len(uncached_texts)} ä¸ªæ–°æ–‡æœ¬")
            
            for i in range(0, len(uncached_texts), batch_size):
                batch = uncached_texts[i:i+batch_size]
                batch_indices = uncached_indices[i:i+batch_size]
                
                headers = {
                    "Authorization": f"Bearer {self.api_key}",
                    "Content-Type": "application/json"
                }
                
                payload = {
                    "model": "GLM-Embedding-3",
                    "input": [t[:512] for t in batch]
                }
                
                try:
                    response = requests.post(
                        self.embed_url,
                        headers=headers,
                        json=payload,
                        timeout=60
                    )
                    response.raise_for_status()
                    result = response.json()
                    
                    if "data" in result:
                        for idx_in_batch, data in enumerate(result["data"]):
                            embedding = data["embedding"]
                            original_idx = batch_indices[idx_in_batch]
                            text_hash = self._get_text_hash(batch[idx_in_batch])
                            self.cache[text_hash] = embedding
                            results[original_idx] = (original_idx, embedding)
                            
                except Exception as e:
                    print(f"  Batch error: {e}")
                    # å¤±è´¥åé€ä¸ªå°è¯•
                    for idx_in_batch, text in enumerate(batch):
                        original_idx = batch_indices[idx_in_batch]
                        emb = self.get_embedding(text)
                        results[original_idx] = (original_idx, emb)
        
        # ä¿å­˜ç¼“å­˜
        self._save_cache()
        
        # è¿”å›æŒ‰åŸå§‹é¡ºåºçš„ç»“æœ
        return [emb for _, emb in sorted(results, key=lambda x: x[0])]


class LoCoMoHybridRetriever:
    """ä¸º LoCoMo å®šåˆ¶çš„ Hybrid Retriever"""
    
    def __init__(self, api_key: str, base_url: str = "https://llmapi.paratera.com"):
        self.embedding_client = CachedEmbeddingClient(api_key, base_url)
        
        # å­˜å‚¨
        self.facts = []
        self.session_dates = {}
        self.bm25_corpus = []
    
    def parse_session_date(self, date_str: str) -> Optional[datetime]:
        """è§£æä¼šè¯æ—¥æœŸ"""
        match = re.search(r'(\d{1,2})[:\s]*(am|pm)?\s*on\s+(\d{1,2})\s+([A-Za-z]+),?\s+(\d{4})', 
                         date_str, re.IGNORECASE)
        if match:
            day = int(match.group(3))
            month_name = match.group(4).lower()
            year = int(match.group(5))
            
            month_map = {
                'january': 1, 'february': 2, 'march': 3, 'april': 4,
                'may': 5, 'june': 6, 'july': 7, 'august': 8,
                'september': 9, 'october': 10, 'november': 11, 'december': 12
            }
            month = month_map.get(month_name)
            if month:
                try:
                    return datetime(year, month, day)
                except:
                    pass
        return None
    
    def build_index(self, data: Dict):
        """æ„å»ºæ··åˆç´¢å¼•"""
        conversation = data.get('conversation', {})
        observation = data.get('observation', {})
        
        # 1. æå–ä¼šè¯æ—¥æœŸ
        for key in conversation.keys():
            if key.endswith('_date_time'):
                session_key = key.replace('_date_time', '')
                parsed = self.parse_session_date(conversation[key])
                if parsed:
                    self.session_dates[session_key] = parsed
        
        print(f"è§£æåˆ° {len(self.session_dates)} ä¸ªä¼šè¯æ—¥æœŸ")
        
        # 2. ä» observation æå–äº‹å®
        # ç»“æ„: session_X_observation -> {person: [[fact, timestamp], ...]}
        fact_texts = []
        for session_key, obs_dict in observation.items():
            session = session_key.replace('_observation', '')
            session_date = self.session_dates.get(session, datetime(2023, 5, 1))
            
            if isinstance(obs_dict, dict):
                for person, fact_list in obs_dict.items():
                    if isinstance(fact_list, list):
                        for fact_item in fact_list:
                            if isinstance(fact_item, list) and len(fact_item) >= 1:
                                fact_text = fact_item[0]
                                if isinstance(fact_text, str) and len(fact_text) > 20:
                                    self.facts.append({
                                        'content': fact_text,
                                        'date': session_date,
                                        'source': 'observation',
                                        'session': session,
                                        'person': person
                                    })
                                    fact_texts.append(fact_text)
        
        print(f"æå–äº† {len(self.facts)} ä¸ªäº‹å®")
        
        # 3. æ‰¹é‡ç”Ÿæˆ embeddings
        print("æ‰¹é‡ç”Ÿæˆ embeddings...")
        embeddings = self.embedding_client.get_embeddings_batch(fact_texts, batch_size=32)
        
        for i, emb in enumerate(embeddings):
            self.facts[i]['embedding'] = emb
        
        print(f"Embeddings å®Œæˆ: {len([f for f in self.facts if f.get('embedding')])}/{len(self.facts)}")
        
        # 4. å‡†å¤‡ BM25
        for fact in self.facts:
            self.bm25_corpus.append(fact['content'].lower().split())
        
        print("ç´¢å¼•æ„å»ºå®Œæˆ!")
    
    def cosine_similarity(self, a: List[float], b: List[float]) -> float:
        """è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦"""
        if not a or not b or len(a) != len(b):
            return 0.0
        
        dot = sum(x * y for x, y in zip(a, b))
        norm_a = math.sqrt(sum(x * x for x in a))
        norm_b = math.sqrt(sum(x * x for x in b))
        
        if norm_a == 0 or norm_b == 0:
            return 0.0
        
        return dot / (norm_a * norm_b)
    
    def bm25_search(self, query: str, top_k: int = 20) -> List[Tuple[int, float]]:
        """BM25 æ£€ç´¢"""
        query_terms = query.lower().split()
        scores = []
        
        for idx, doc in enumerate(self.bm25_corpus):
            score = 0
            for term in query_terms:
                if term in doc:
                    tf = doc.count(term) / len(doc) if doc else 0
                    score += tf
            if score > 0:
                scores.append((idx, score))
        
        scores.sort(key=lambda x: -x[1])
        return scores[:top_k]
    
    def vector_search(self, query: str, top_k: int = 20) -> List[Tuple[int, float]]:
        """å‘é‡æ£€ç´¢"""
        query_emb = self.embedding_client.get_embedding(query)
        if not query_emb:
            return []
        
        scores = []
        for idx, fact in enumerate(self.facts):
            if fact.get('embedding'):
                sim = self.cosine_similarity(query_emb, fact['embedding'])
                if sim > 0.3:  # é™ä½é˜ˆå€¼
                    scores.append((idx, sim))
        
        scores.sort(key=lambda x: -x[1])
        return scores[:top_k]
    
    def rrf_fusion(self, bm25_results: List[Tuple[int, float]], 
                   vector_results: List[Tuple[int, float]], 
                   k: int = 60) -> List[Tuple[int, float]]:
        """RRF èåˆ"""
        scores = {}
        
        for rank, (idx, _) in enumerate(bm25_results):
            scores[idx] = scores.get(idx, 0) + 1.0 / (k + rank + 1)
        
        for rank, (idx, _) in enumerate(vector_results):
            scores[idx] = scores.get(idx, 0) + 1.0 / (k + rank + 1)
        
        return sorted(scores.items(), key=lambda x: -x[1])
    
    def retrieve(self, query: str, top_k: int = 5) -> List[Dict]:
        """æ··åˆæ£€ç´¢"""
        bm25_results = self.bm25_search(query, top_k=20)
        vector_results = self.vector_search(query, top_k=20)
        
        fused_results = self.rrf_fusion(bm25_results, vector_results)
        
        results = []
        for idx, score in fused_results[:top_k]:
            if 0 <= idx < len(self.facts):
                fact = self.facts[idx].copy()
                fact['retrieval_score'] = score
                results.append(fact)
        
        return results
    
    def answer_when(self, question: str) -> str:
        """å›ç­” When é—®é¢˜"""
        results = self.retrieve(question, top_k=10)
        
        dated_facts = [r for r in results if r.get('date')]
        
        if dated_facts:
            best = dated_facts[0]
            return best['date'].strftime('%d %B %Y')
        
        return "Unknown"


def calculate_f1(predicted: str, ground_truth: Any) -> float:
    """è®¡ç®— F1"""
    if isinstance(ground_truth, (int, float)):
        ground_truth = str(ground_truth)
    
    pred = str(predicted).lower().strip()
    truth = str(ground_truth).lower().strip()
    
    if pred == truth:
        return 1.0
    
    if truth in pred or pred in truth:
        return 0.8
    
    pred_year = re.search(r'\b(20\d{2})\b', pred)
    truth_year = re.search(r'\b(20\d{2})\b', truth)
    if pred_year and truth_year:
        if pred_year.group(1) == truth_year.group(1):
            return 0.7
    
    pred_chars = set(pred)
    truth_chars = set(truth)
    
    if not pred_chars or not truth_chars:
        return 0.0
    
    intersection = pred_chars & truth_chars
    precision = len(intersection) / len(pred_chars) if pred_chars else 0
    recall = len(intersection) / len(truth_chars) if truth_chars else 0
    
    if precision + recall == 0:
        return 0.0
    
    return 2 * precision * recall / (precision + recall)


def main():
    print("="*70)
    print("LoCoMo Hybrid Retriever - æ‰¹å¤„ç† + ç¼“å­˜ç‰ˆæœ¬")
    print("="*70)
    
    # åŠ è½½æ•°æ®
    with open('/tmp/mimir-review/mimir-native/locomodata.json', 'r') as f:
        data = json.load(f)
    
    conv = data[0]
    qa_list = conv.get('qa', [])
    
    # æ£€æŸ¥ç¼“å­˜
    cache_file = '/tmp/mimir-review/mimir-native/embeddings_cache.json'
    cache_exists = os.path.exists(cache_file)
    if cache_exists:
        cache_size = os.path.getsize(cache_file)
        print(f"\nå‘ç°ç¼“å­˜æ–‡ä»¶: {cache_file} ({cache_size/1024:.1f} KB)")
    
    # åˆå§‹åŒ– Hybrid Retriever
    retriever = LoCoMoHybridRetriever(
        api_key="sk-0oVqiF3DzxzxTcbxsaPEOg",
        base_url="https://llmapi.paratera.com"
    )
    
    # æ„å»ºç´¢å¼•
    print("\næ„å»º Hybrid Index...")
    retriever.build_index(conv)
    
    # ç­›é€‰ When é—®é¢˜
    when_questions = [(i, qa) for i, qa in enumerate(qa_list) 
                     if qa.get('question', '').lower().startswith('when')]
    
    print(f"\næµ‹è¯• {len(when_questions)} ä¸ª When é—®é¢˜...")
    print("="*70)
    
    results = []
    for idx, qa in when_questions:
        question = qa['question']
        ground_truth = qa['answer']
        
        predicted = retriever.answer_when(question)
        f1 = calculate_f1(predicted, ground_truth)
        
        results.append({
            'q_id': idx + 1,
            'question': question,
            'predicted': predicted,
            'ground_truth': str(ground_truth),
            'f1': f1
        })
        
        status = "âœ“" if f1 >= 0.8 else "~" if f1 >= 0.5 else "âœ—"
        print(f"  [{idx+1:3d}] {status} F1:{f1:.0%}")
        print(f"        Q: {question[:50]}...")
        print(f"        A: {predicted[:30]:30s} | çœŸå®: {str(ground_truth)[:30]}...")
    
    # ç»Ÿè®¡
    avg_f1 = sum(r['f1'] for r in results) / len(results) if results else 0
    correct = sum(1 for r in results if r['f1'] >= 0.8)
    partial = sum(1 for r in results if 0.5 <= r['f1'] < 0.8)
    wrong = sum(1 for r in results if r['f1'] < 0.5)
    
    print(f"\n{'='*70}")
    print(f"æ­£ç¡®: {correct}, éƒ¨åˆ†: {partial}, é”™è¯¯: {wrong}")
    print(f"When é—®é¢˜ F1: {avg_f1:.2%}")
    print(f"{'='*70}")
    
    # å¯¹æ¯”
    print("\nğŸ“Š å¯¹æ¯”:")
    print(f"  åŸå§‹ç‰ˆ:        25.3%")
    print(f"  SessionåŒ¹é…ç‰ˆ: 69.2%")
    print(f"  Hybridæ£€ç´¢ç‰ˆ:  {avg_f1:.1%}")
    
    # ä¿å­˜ç»“æœ
    output = {
        'timestamp': datetime.now().isoformat(),
        'method': 'Hybrid Retriever (BM25 + Embedding + RRF + Cache)',
        'num_when_questions': len(when_questions),
        'avg_f1': avg_f1,
        'results': results
    }
    
    output_path = f"/tmp/mimir-review/mimir-native/locomo_hybrid_cached_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
    with open(output_path, 'w') as f:
        json.dump(output, f, indent=2)
    
    print(f"\nç»“æœå·²ä¿å­˜: {output_path}")
    
    # æ˜¾ç¤ºç¼“å­˜ç»Ÿè®¡
    if os.path.exists(cache_file):
        with open(cache_file, 'r') as f:
            cache = json.load(f)
        print(f"ç¼“å­˜ç»Ÿè®¡: {len(cache)} ä¸ª embeddings å·²ç¼“å­˜")


if __name__ == "__main__":
    main()
